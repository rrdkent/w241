---
title: "Problem Set 2, Experiments and Causality"
author: "Daniel Kent, Spring 2019"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
  df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE}
library(data.table)
```
<!--

Some guidelines for submitting problem sets in this course:

- Please submit a PDF document rather than a Word document or a Google document.
- Please put your name at the top of your problem set.
- Please **bold** or *highlight* your numerical answers to make them easier to find.
- If you'll be using `R` or `Python` code to calculate your answers, please put the code and its output directly into your Problem Set PDF document.
- It is highly recommended, although not required, that you use the RMarkdown feature in RStudio to compose your problem set answers. RMarkdown allows you to easily intermingle analysis code and answers in one document. It is of a similar design as `jupyter` and an ipython notebook.
- You do not need to show work for trivial calculations, but showing work is always allowed.
- For answers that involve a narrative response, please feel free to describe the key concept directly and briefly, if you can do so, and do not feel pressure to go on at length.
- Please ask us questions about the problem set if you get stuck. **Don't spend more than 20 minutes puzzling over what a problem means.** 
- Please ensure that someone (us!) can compile your solution set. The best way is to use the web-hosted links we've provided. 
--> 

# 1. What happens when pilgrims attend the Hajj pilgrimage to Mecca? 

On the one hand, participating in a common task with a diverse group of pilgrims might lead to increased mutual regard through processes identified in *Contact Theories*. On the other hand, media narritives have raised the spectre that this might be accompanied by "antipathy toward non-Muslims". [Clingingsmith, Khwaja and Kremer (2009)](https://dash.harvard.edu/handle/1/3659699) investigates the question. 

Using the data here, test the sharp null hypothesis that winning the visa lottery for the pilgrimage to Mecca had no effect on the views of Pakistani Muslims toward people from other countries. Assume that the Pakistani authorities assigned visas using complete random assignment. Use, as your primary outcome the `views` variable, and as your treatment feature `success`. If you're ambitious, write your fucntion generally so that you can also evaluate feeligns toward specific nationalities.

```{r}
d <- read.csv("./data/Clingingsmith.2009.csv", stringsAsFactors = FALSE)
dt <- data.table(d)
```
a. Using either `dplyr` or `data.table`, group the data by `success` and report whether views toward others are generally more positive among lottery winners or lottery non-winners. 

**We recall from the Datacamp Unit on data.table that the format for datatable is i, j, by, or in SQL-speak, WHERE, SELECT, GROUPBY.  Consequently, we want to groupby success and measure the average view.  That is, we want to get the mean of the views, grouped by success.  Or said formally, **

```{r}
dt[,.(mean = mean(views)),by = success]
```

**From these data, we do see that, across all observations, those individuals who had "success" (1) with the lottery held a more positive view, in aggregate, than those who "did not have success" (0) to a degree of approximately 2.34 to 1.87**

```{r}
true_ate <- dt[,.(mean = mean(views)),by = success]$mean[2]-dt[,.(mean = mean(views)),by = success]$mean[1]
true_ate
```

**We observe that the true ATE is approximately .48**

b. But is this a meaningful difference, or could it just be randomization noise? Conduct 10,000 simulated random assignments under the sharp null hypothesis to find out. (Don't just copy the code from the async, think about how to write this yourself.) 

**OK, first we want to see how many of each (success and no-success) do we have, just out of curiosity.**
```{r}
dt[, .(count = .N), by=success]
```

**We see that it's very roughly 50/50**


**We'll take 30% of the total number of records.**

```{r}
dt[,.(count = .N),]*.3
```

**So our sample will consist of 288 records.**


**Our Sharpe Null hypothesis is that there is a $\tau$ of no change, that is, $H_A-H_0=0$, where $H_A$ refers to the success of getting a visa, and $H_0$ refers to not being successful in getting a visa.  **

**My proceedure will be the following:**
**Create a function that: **
**  Randomly draw 288 records**
**  Stores in a vector the number of successes for each group**
**  calculate each group's view mean.**
**  calculate ate-hat by subtracting loss from success**
**  store ate-hat into a vector**

```{r}
#Create a function:
run_study <- function(){
  # pull a sample & store it in a vector
  my_sample <- as.data.table(sapply(dt[], sample, 288))
  #return the number of successes
  n_successes <- my_sample[, .(count = .N), by=success]$count[1]
  n_successes
  #calculate ate-hat by subtracting sample mean loss from mean success
  ate <- my_sample[,.(mean = mean(views)),by = success]$mean[2] - my_sample[,.(mean = mean(views)),by = success]$mean[1]
  #return ate-hat
  ate
  #my_sample
}
```

```{r}
run_study()
```
**Now, let's replicate the proceedure**

```{r}
results <- t(replicate(10000, run_study()))
```

c. How many of the simulated random assignments generate an estimated ATE that is at least as large as the actual estimate of the ATE? 

```{r}
sum(results >= true_ate)
```

**520 of the simulated random assignments generated an estimated ATE that is at least as large as the actual estimate of the ATE.**

d. What is the implied *one-tailed* p-value? 

**We recall that we can get the p-value under the randomization inference framework, by examining hte precent of values that occur at a place that is smaller or larger than the p-value we observe by taking the mean of the true ATE and the distribution under the sharpe null. **

```{r}
mean(true_ate <results )
```

**We see that this is boarderline signficant at .055.**

e. How many of the simulated random assignments generate an estimated ATE that is at least as large *in absolute value* as the actual estimate of the ATE? 

```{r}
sum(abs(results) >= true_ate)
```

**1099 random assignments are at least as large in aboslute value as the actual estimate of the ATE.**

f. What is the implied two-tailed p-value? 

**My understanding is that to take the two-tailed p-value, we double the value of the one-tailed p-value.**

```{r}
mean(true_ate <results )*2
```

**0.11 is not significant and means that we do not reject the Sharpe Null Hypothesis.**

# 2. Term Limits Aren't Good. 

Naturally occurring experiments sometimes involve what is, in effect, block random assignment. For example, [Rocio Titiunik](https://sites.google.com/a/umich.edu/titiunik/publications) , in [this paper](http://www-personal.umich.edu/~titiunik/papers/Titiunik2016-PSRM.pdf) studies the effect of lotteries that determine whether state senators in TX and AR serve two-year or four-year terms in the aftermath of decennial redistricting. These lotteries are conducted within each state, and so there are effectively two distinct experiments on the effects of term length.

The "thoery" in the news (such as it is), is that legislators who serve 4 year terms have more time to slack off and not produce legislation. If this were true, then it would stand to reason that making terms shorter would increase legislative production. 

One way to measure legislative production is to count the number of bills (legislative proposals) that each senator introduces during a legislative session. The table below lists the number of bills introduced by senators in both states during 2003. 

```{r}
library(foreign)

d <- read.dta("./data/Titiunik.2010.dta")
head(d)
```

```{r}
#load it up into data table
dt <- data.table(d)
```


a. Using either `dplyr` or `data.table`, group the data by state and report the mean number of bills introduced in each state. Does Texas or Arkansas seem to be more productive? Then, group by two- or four-year terms (ignoring states). Do two- or four-year terms seem to be more productive? **Which of these effects is causal, and which is not?** Finally, using `dplyr` or `data.table` to group by state and term-length. How, if at all, does this change what you learn? 

**Per the framework, we want to take dt, calculate the mean number of bills, grouped by state**

```{r}
dt[,.(mean = mean(bills_introduced)),by = texas0_arkansas1]
```
**From these results, we see that by taking the mean number of bills introduced in each state, Texas seems to be more productive with a mean of approximately 68.8 than Arkansas with a mean of approximately 25.5.**


**Per the framework, we want to take dt again, calculate the mean number of bills, grouped by two- or four-year terms.**

```{r}
dt[,.(mean = mean(bills_introduced)),by = term2year]
```
**From these results, we see that by taking the mean number of bills and grouping them by two years (1) or four years (0), four year terms (0) have a greater mean number of bills introduced in each state at approximately 53, versus two year terms (1) with a mean number of bills introduced of approximately 38.6.**


**Finally, per the data table framework, we want to take dt again, calculate the mean number of bills grouped by state AND two- or four-year terms.**

```{r}
dt[,.(mean = mean(bills_introduced)),by = list(texas0_arkansas1,term2year)]
```

**From these data, we still see a similar story, though the data are more clear.  For the state of Texas, we see that there is still a greater mean number of bills (average approximately 76.9) introduced per legislator where term limits are four years (0), versus when term limits are two years (1) (average approximately 60.1).  Similarly, for the state of Arkansas, when term limits are four years (0), we observe a greater mean number of bills introduced by legislators (average approximately 30.7) versus when term limits are two years (1) (average approximately 20.6).  With these data, we can see that there is a general difference between the number of bills introduced by legislators in each state but also in context of the terms. **

b. For each state, estimate the standard error of the estimated ATE. 

**From 3.6, we calculate ** $\hat{SE} = \sqrt{\frac{\hat{Var(Y_i(0))}}{N-m}+\frac{\hat{Var(Y_i(1))}}{m}}$

**Where m is the number of observations from the treatment group**

```{r}
#Set up values:
AR_Y1 <- (dt[which(texas0_arkansas1==1 & term2year==1),,]$bills_introduced)
AR_Y0 <- (dt[which(texas0_arkansas1==1 & term2year==0),,]$bills_introduced)
TX_Y1 <- (dt[which(texas0_arkansas1==0 & term2year==1),,]$bills_introduced)
TX_Y0 <- (dt[which(texas0_arkansas1==0 & term2year==0),,]$bills_introduced)

AR_Y1_n <- (dt[which(texas0_arkansas1==1 & term2year==1),.(count = .N),])
AR_Y0_n <- (dt[which(texas0_arkansas1==1 & term2year==0),.(count = .N),])
TX_Y1_n <- (dt[which(texas0_arkansas1==0 & term2year==1),.(count = .N),])
TX_Y0_n <- (dt[which(texas0_arkansas1==0 & term2year==0),.(count = .N),])


tot_obs_AR <- AR_Y1_n+AR_Y0_n
tot_obs_TX <- TX_Y1_n+TX_Y0_n
tot_obs <-dt[, .(count = .N),]

##calculate variances:
AR_var_Y1 <- var(AR_Y1)
AR_var_Y0 <- var(AR_Y0)
TX_var_Y1 <- var(TX_Y1)
TX_var_Y0 <- var(TX_Y0)


#Calculate AR Standard Error
AR_SE <- sqrt(((AR_var_Y0)/(AR_Y0_n))+((AR_var_Y1)/(AR_Y1_n)))
#Calculate TX Standard Error:
TX_SE <- sqrt(((TX_var_Y0)/(TX_Y0_n))+((TX_var_Y1)/(TX_Y1_n)))
print(c("Arkansas Standard Error", AR_SE, "Texas Standard Error", TX_SE))
```

**Arkansas's standard error is approximately 3.4 and Texas's standard error is approixmately 9.35.**

c. Use equation (3.10) to estimate the overall ATE for both states combined. 

**We use the equation 3.10,**

$ATE = \sum_{j=1}^{J}\frac{N_j}{N}ATE_j$

**Where J is the number of blocks, j is the blocks' index, and the weight $N_j/N$ is the share of subjects who belong to block j**

```{r}

AR_ATE <- mean(AR_Y1)-mean(AR_Y0)
TX_ATE <- mean(TX_Y1)-mean(TX_Y0)

tot_ATE <- sum((tot_obs_AR/tot_obs)*AR_ATE, (tot_obs_TX/tot_obs)*TX_ATE)

tot_ATE

```

**The estimated overall ATE for both states combined is approximately -13.22.**


d. Explain why, in this study, simply pooling the data for the two states and comparing the average number of bills introduced by two-year senators to the average number of bills introduced by four-year senators leads to biased estimate of the overall ATE. 

**Intuitively, I would have suspected, ceteris paribus, that there would not be any affect on the overall ATE.  But thinking about it futher, perhaps pooling the data from the two states introduced by two-year senators compared to four-year senators leads to a biased estimate of the overall ATE chiefly because each state has different magnitudes of effects and combining them muddles the overall ATE as each state is clearly different (with Arkansas having a significantly lower base-line of bills introduced).  **

e. Insert the estimated standard errors into equation (3.12) to estimate the stand error for the overall ATE. 

$SE(\hat{ATE}) = \sqrt{(SE_1)^2(\frac{N_1}{N})^2+(SE_2)^2(\frac{N_2}{N})^2}$

```{r}
SE_ATE <- sqrt((AR_SE)^2*(AR_Y1_n+AR_Y0_n/tot_obs)^2+(TX_SE)^2*(TX_Y1_n+TX_Y0_n/tot_obs)^2)
SE_ATE
```

**After, inserting the estimated standard errors into equation (3.12) to estimate the standard error for the overall ATE, we get a standard error of approximately 155.36.**

f. Use randomization inference to test the sharp null hypothesis that the treatment effect is zero for senators in both states. 

**Our Sharpe Null hypothesis is that there is a $\tau$ of no change, that is, $H_A-H_0=0$, where $H_A$ refers to the reduction of a senator's term to 2 years, and $H_0$ refers to maintaining the senator's term to 4 years..  **

**My proceedure will be the following:**
**Create a function that: **
**  Randomly draw 18 records**
**  Stores in a vector the number of successes for each group**
**  calculate each group's mean.**
**  calculate ate-hat by subtracting control mean from treatment mean**
**  store ate-hat into a vector**
****
```{r}
run_study <- function(state){
  # pull a sample & store it in a vector.  I'm choosing samples of 18.
  my_sample <- as.data.table(sapply((dt[which(texas0_arkansas1==state),,]), sample, 18))
  #calculate ate-hat by subtracting sample mean loss from mean success
  ate_hat <- my_sample[,.(mean = mean(bills_introduced)),by = term2year]$mean[2] - my_sample[,.(mean = mean(bills_introduced)),by = term2year]$mean[1]
  #return ate-hat
  ate_hat
}
``` 

```{r}
run_study(0)
```
```{r}
dt[,.(mean = mean(bills_introduced)),by = term2year]$mean[2] - dt[,.(mean = mean(bills_introduced)),by = term2year]$mean[1]
```


**Now, let's replicate each experiment 1000 times.**

```{r}
AR_results <- t(replicate(1000, run_study(1)))
TX_results <- t(replicate(1000, run_study(0)))
```

```{r}
plot(density(TX_results))
```

```{r}
plot(density(AR_results))
```


**Now let's calculate the true ATE for Texas and True ATE for Arkansas:**

```{r}
TX_true_ATE <- (dt[which(texas0_arkansas1==0 & term2year==1),.(mean(bills_introduced)),] - dt[which(texas0_arkansas1==0 & term2year==0),.(mean(bills_introduced)),])$V1
TX_true_ATE
```
**Our true ATE for Texas is approximately -16.74.**


```{r}
AR_true_ATE <-(dt[which(texas0_arkansas1==1 & term2year==1),.(mean(bills_introduced)),] - dt[which(texas0_arkansas1==1 & term2year==0),.(mean(bills_introduced)),])$V1
AR_true_ATE
```
**Our true ATE for Arkansas is approximately -10.1.**

To test the Sharp null, we calculate the implied two-tailed p-value to see if we can reject or accept the null.

**Now we calculate the P-Value**

```{r}
TX_p_val <-mean(TX_true_ATE > TX_results )
TX_p_val
```

**With a p-value of 0.12, we cannot reject the Sharp null hypothesis for Texas, i.e. there is no average treatment effect.**

```{r}
AR_p_val <-mean(AR_true_ATE > AR_results )
AR_p_val
```


**However, with Arkansas, we get a p-value of .02 which is statistically significant, meaning that we can reject the Sharp null hypothesis of no average treatment effect leading us to conclude that there is an average treatment effect. **


**What if we do it as a proportion?**

```{r}
true_ate_prop <- TX_p_val*(tot_obs_TX/tot_obs) + AR_p_val*(tot_obs_AR/tot_obs)
true_ate_prop

```

**If done as a proportion, we get a p-value of approximately .08, which is marginally significant.**

**What if we run the sampling together?**

```{r}
#Run combined study

run_comb_study <- function(){
  # pull a sample & store it in a vector.  I'm choosing samples of 18.
  my_sample <- as.data.table(sapply((dt[,,]), sample, 18))
  #calculate ate-hat by subtracting sample mean loss from mean success
  ate_hat <- my_sample[,.(mean = mean(bills_introduced)),by = term2year]$mean[2] - my_sample[,.(mean = mean(bills_introduced)),by = term2year]$mean[1]
  #return ate-hat
  ate_hat
}

```
```{r}
run_comb_study()
```

```{r}
comb_study <- t(replicate(1000, run_comb_study()))
```


```{r}
plot(density(comb_study))
```

```{r}
mean(tot_ATE > comb_study )
```

**Taken together, with a p-value of approximately 0.18, we cannot reject the Sharp null hypothesis for Texas, i.e. there is no average treatment effect.**

g. **IN Addition:** Plot histograms for both the treatment and control groups in each state (for 4 histograms in total).


```{r}
AR_Y1_results <- t(replicate(1000, mean(dt[which(texas0_arkansas1==1 & term2year==1),,]$bills_introduced))) 
AR_Y0_results <- t(replicate(1000, mean(dt[which(texas0_arkansas1==1 & term2year==0),,]$bills_introduced))) 

TX_Y1_results <- t(replicate(1000, mean(dt[which(texas0_arkansas1==0 & term2year==1),,]$bills_introduced))) 
TX_Y0_results <- t(replicate(1000, mean(dt[which(texas0_arkansas1==0 & term2year==0),,]$bills_introduced))) 
```




```{r}
plot(density(AR_Y1_results), main = "Arkansas Treatment")
```

```{r}
plot(density(AR_Y0_results), main = "Arkansas Control")
```

```{r}
plot(density(TX_Y1_results), main = "Texas Treatment")
```

```{r}
plot(density(TX_Y0_results), main = "Texas Control")
```

# 3. Cluster Randomization
Use the data in *Field Experiments* Table 3.3 to simulate cluster randomized assignment. (*Notes: (a) Assume 3 clusters in treatment and 4 in control; and (b) When Gerber and Green say ``simulate'', they do not mean ``run simulations with R code'', but rather, in a casual sense ``take a look at what happens if you do this this way.'' There is no randomization inference necessary to complete this problem.*)


```{r}
## load data 
d <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
```

```{r}
#datatable4eva
dt <- data.table(d)
dt
```


a. Suppose the clusters are formed by grouping observations {1,2}, {3,4}, {5,6}, ... , {13,14}. Use equation (3.22) to calculate the standard error assuming half of the clusters are randomly assigned to treatment. 

**We recall that 3.22 says the following:**

$SE(\hat{ATE})=\sqrt{\frac{1}{k-1}\left\{ \frac{mVar(Y_j(0))}{N-m}+\frac{(N-m)Var(Y_j(1))}{m}+2Cov(\bar{Y}_j(0),\bar{Y}_j(1)) \right\} }$

```{r} 
#There are fourteen observations, so by matching them, we will get 7 clusters.  We will randomly select 3 clusters to get treatment and 3 to be the control.  One will be held-out and get to relax with a frothy beverage because covariance must be calculated with similar-size matricies.

#First lets see our sample (the first three for control, the last three for treatment)
sample(x = 1:7, size = 6)
```
**We will pick clusters 3, 6, and 2 for control, and 1, 7, and 4 for treatment.**

```{r}

treat <- dt[which(Village==5 | Village==6 | Village==11 | Village==12 | Village==3 | Village==4),,]
control <- dt[which(Village==1 | Village==2 | Village==13 | Village==14 | Village==7 | Village==8),,] 


c1 <- dt[which(Village==1 | Village==2),,]
c2 <- dt[which(Village==3 | Village==4),,]
c3 <- dt[which(Village==5 | Village==6),,]
c4 <- dt[which(Village==7 | Village==8),,]
c5 <- dt[which(Village==9 | Village==10),,]
c6 <- dt[which(Village==11 | Village==12),,]
c7 <- dt[which(Village==13 | Village==14),,]

```

**Now that we have our treatment and control clusters grouped, we will now begin the calculations for the SE**

```{r}
#calculate variances:
treat_var <- var(treat$Y)
control_var <- var(control$D)

#calculate covariance:
cov <- cov(control$D,treat$Y)

#Number of clusters
k = 7
#m:
m <- 6
#N-m
N_m <-6

SE_ATE <- sqrt(((1)/(k-1))*((m*control_var)/(N_m))+(((N_m)*control_var)/(m))+(2*cov))
SE_ATE

```

```{r}
print(c("treat var", treat_var, "control var", control_var, "cov", cov))
```


**Our standard error for this set of calculations is 10.61838.**

b. Suppose that clusters are instead formed by grouping observations {1,14}, {2,13}, {3,12}, ... , {7,8}. Use equation (3.22) to calculate the standard error assuming half of the clusters are randomly assigned to treatment. 

```{r}
#Similar to above, there are fourteen observations, so by matching them, we will get 7 clusters.  We will randomly select 3 clusters to get treatment and 3 to be the control.  One will be held-out again because covariance must be calculated with similar-size matricies.

#First lets see our sample (the first three for control, the last three for treatment)
sample(x = 1:7, size = 6)
```
**We will pick clusters 6, 4, and 7 for control, and 2, 1, and 3 for treatment.**



```{r} 
treat <- dt[which(Village==6 | Village==9 | Village==4 | Village==11 | Village==7 | Village==8),,]
control <- dt[which(Village==2 | Village==13 | Village==1 | Village==14 | Village==3 | Village==12),,] 


c1 <- dt[which(Village==1 | Village==14),,]
c2 <- dt[which(Village==2 | Village==13),,]
c3 <- dt[which(Village==3 | Village==12),,]
c4 <- dt[which(Village==4 | Village==11),,]
c5 <- dt[which(Village==5 | Village==10),,]
c6 <- dt[which(Village==6 | Village==9),,]
c7 <- dt[which(Village==7 | Village==8),,]
``` 

**Now that we have our treatment and control clusters grouped, we will now begin the calculations for the SE**

```{r}
#calculate variances:
treat_var <- var(treat$Y)
control_var <- var(control$D)

#calculate covariance:
cov <- cov(control$D,treat$Y)

#Number of clusters
k = 7
#m:
m <- 6
#N-m
N_m <-6

SE_ATE <- sqrt(((1)/(k-1))*((m*control_var)/(N_m))+(((N_m)*control_var)/(m))+(2*cov))
SE_ATE

```
```{r}
print(c("treat var", treat_var, "control var", control_var, "cov", cov))
```

**Our standard error for this set of calculations is 11.12255.**

c. Why do the two methods of forming clusters lead to different standard errors? What are the implications for the design of cluster randomized experiments? 

**The first clustering had a lower standard error than the second clustering and my intuittion is that it because the first clustering had values that were more similar to eachother versus the second clustering, which was more random.  Perhaps if there is a smaller total variance, because the first clustering had a total treatment and control variance sum of 81.1 and the second clustering had a total treatment and control variance sum of 83.23.**

# 4. Sell Phones? 
You are an employee of a newspaper and are planning an experiment to demonstrate to Apple that online advertising on your website causes people to buy iPhones. Each site visitor shown the ad campaign is exposed to $0.10 worth of advertising for iPhones. (Assume all users could see ads.) There are 1,000,000 users available to be shown ads on your newspaper's website during the one week campaign. 

Apple indicates that they make a profit of $100 every time an iPhone sells and that 0.5% of visitors to your newspaper's website buy an iPhone in a given week in general, in the absence of any advertising.

a. By how much does the ad campaign need to increase the probability of purchase in order to be "worth it" and a positive ROI (supposing there are no long-run effects and all the effects are measured within that week)?


<!-- Carson Says:
Basically the two questions you need to answer are how much is being spent on ads currently
and how does an increase in purchase probability change the amount of profit taking in
I think answering the first of the above should help you clarify the problem
see if you can construct the equation: ad cost = lift in profit
(you might have a version of that above, but I'm not quite following what you have there)

!-->

**Currently Apple is spending nothing, but makes** $100*.005=0.5$ **per user.**

**From the perspective of Apple**
$users*conversionrate*unit_profit-(cpm * users)=0$
$1000000*conversionrate*100-(.1*1000000)=0$
$1000000*conversionrate*100-(100000)=0$
$1000000*conversionrate*100=100000$
$1000000*conversionrate=1000$
$conversionrate = .001$

**Seen another way:**

**Profit per user = Revenue per user - Costs per user**

**Cost (per user) = $0.10**

**Revenue (per user) = $100 * .001 % = $0.10**

```{r}
#Cost:
cpi <- .1
users <- 1
Ad_spend <- users *cpi
Ad_spend
```

**Adspend right now is 100K**

```{r}
#Revenue:
conversion <-.001
unit_profit <-100
revenue <- users * conversion * unit_profit
revenue
```

**For things to be worth it, the ads need to provide a .1 lift in probability of a user buying an iPhone to the existing .5% for a total conversion rate of .6%.**

b. Assume the measured effect is 0.2 percentage points. If users are split 50:50 between the treatment group (exposed to iPhone ads) and control group (exposed to unrelated advertising or nothing; something you can assume has no effect), what will be the confidence interval of your estimate on whether people purchase the phone?

**If we use the entire audience population of 1 million:**

**We then plug in the p into the SE equation:**

$\sqrt{\frac{x_{1}+x_{2}}{n_{1}+n_{2}}(1-\frac{x_{1}+x_{2}}{n_{1}+n_{2}})*(\frac{1}{n_{1}}+\frac{1}{n_{2}})}$

```{r}
coeff <- .002

users <- 1000000
x_1 <- (.005) * users #successes = rate(base) * users
x_2 <- (.002+.005) * users #successes = rate(base+new) * users
n_1 <- users/2
n_2 <- users/2

SE <- sqrt(((x_1+x_2)/(n_1+n_2))*(1-((x_1+x_2)/(n_1+n_2)))*(((1)/(n_1))+((1)/(n_2))))

lower_bound <- coeff - stand_error * 1.96
upper_bound <- coeff + stand_error * 1.96
print(c("Confidence Intervial", lower_bound, "to", upper_bound))
```

```{r}
SE
```


  + **Note:** The standard error for a two-sample proportion test is $\sqrt{p(1-p)*(\frac{1}{n_{1}}+\frac{1}{n_{2}})}$ where $p=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$, where $x$ and $n$ refer to the number of "successes" (here, purchases) over the number of "trials" (here, site visits). The length of each tail of a 95% confidence interval is calculated by multiplying the standard error by 1.96.
  
c. Is this confidence interval precise enough that you would recommend running this experiment? Why or why not?

**I'm not 100% sure, but I think that the interval is not precise enough, because our confidence interval is only approximately -.27 to approximately .27 but our standard error is approximately 1.2.**

d. Your boss at the newspaper, worried about potential loss of revenue, says he is not willing to hold back a control group any larger than 1% of users. What would be the width of the confidence interval for this experiment if only 1% of users were placed in the control group?

```{r}
coeff <- .002

users <- 1000000
x_1 <- (.005) * users #successes = rate(base) * users
x_2 <- (.002+.005) * users #successes = rate(base+new) * users
n_1 <- users*.01
n_2 <- 1-n_1

SE <- sqrt(((x_1+x_2)/(n_1+n_2))*(1-((x_1+x_2)/(n_1+n_2)))*(((1)/(n_1))+((1)/(n_2))))

lower_bound <- coeff - stand_error * 1.96
upper_bound <- coeff + stand_error * 1.96
print(c("Confidence Intervial Width", upper_bound-lower_bound))
```

# 5. Sports Cards
Here you will find a set of data from an auction experiment by John List and David Lucking-Reiley ([2000](https://drive.google.com/file/d/0BxwM1dZBYvxBNThsWmFsY1AyNEE/view?usp=sharing)).  

```{r}
d2 <- read.csv("./data/listData.csv", stringsAsFactors = FALSE)
head(d2)
```

In this experiment, the experimenters invited consumers at a sports card trading show to bid against one other bidder for a pair trading cards.  We abstract from the multi-unit-auction details here, and simply state that the treatment auction format was theoretically predicted to produce lower bids than the control auction format.  We provide you a relevant subset of data from the experiment.

a. Compute a 95% confidence interval for the difference between the treatment mean and the control mean, using analytic formulas for a two-sample t-test from your earlier statistics course. 

**For this problem, our treatment is a uniform price auction (1) and the control is a non-uniform price auction (0)**

```{r}
#use data.table to get just the uniform price auction data (treatment):

dt <-data.table(d2)
treatment <- dt[uniform_price_auction == 1,,]

#and now the non-uniform price auction (control):

control <- dt[uniform_price_auction == 0,,]
```

```{r}
t_test <- t.test( control, treatment, conf=0.95)
t_test
```
```{r}
t_test$conf.int
```
 **Our 95% confidence interval is approximately -0.21 and approximately 11.42.**
 
b. In plain language, what does this confidence interval mean?



c. Regression on a binary treatment variable turns out to give one the same answer as the standard analytic formula you just used.  Demonstrate this by regressing the bid on a binary variable equal to 0 for the control auction and 1 for the treatment auction.

```{r}
fit <- lm(treatment$bid ~ control$bid, data=dt )
```

```{r}
linear_model <- summary(fit)
linear_model
```

d. Calculate the 95% confidence interval you get from the regression.

```{r}
linear_model$coefficients
```

**While Knitting, I get some funky error when calling them directly, so I'll manually input them so that knit works.**

```{r}
coeff <- .001116201#linear_model$coefficients[2]
stand_error <- .1362625#regression$coefficients[4]
lower_bound <- coeff - stand_error * 1.96
upper_bound <- coeff + stand_error * 1.96
print(c("Confidence Intervial", lower_bound, "to", upper_bound))
```

e. On to p-values. What p-value does the regression report? Note: please use two-tailed tests for the entire problem.

```{r}
(1-linear_model$coefficients[2,4])*2
```

f. Now compute the same p-value using randomization inference.

```{r}
#Calculate Sample size, 30% of total.
dt[,.N,]*.3
```


**Our Sharpe Null hypothesis is that there is a $\tau$ of no change, that is, $H_A-H_0=0$, where $H_A$ refers to price of a card in a uniform auction, and $H_0$ refers to price of a card i a non-uniform auction.  **

**My proceedure will be the following:**
**Create a function that: **
**  Randomly draw 21 records**
**  calculate each group's view mean.**
**  calculate ate-hat by subtracting the mean of the control from the mean of the treatment **
**  store ate-hat into a vector**

```{r}
dt[,.(mean = mean(bid)),by = uniform_price_auction]
```


```{r}
#Create a function:
run_study <- function(){
  # pull a sample & store it in a vector
  my_sample <- as.data.table(sapply(dt[], sample, 21))
  #calculate ate-hat by subtracting sample mean non-uniform price auction from mean uniform price auction
  ate <- my_sample[,.(mean = mean(bid)),by = uniform_price_auction]$mean[1] - my_sample[,.(mean = mean(bid)),by = uniform_price_auction]$mean[2]
  #return ate-hat
  ate
  #my_sample
}
```

```{r}
run_study()
```
**Now, let's replicate the proceedure**

```{r}
results <- t(replicate(1000, run_study()))
```

```{r}
#let's see what the distribution looks like.
plot(density(results))
```

**And where is our mean in this distribution?**
```{r}
mean(results)
```
**Let's Calculate the one-tailed p-value**
```{r}
#True ATE:
#use data.table to get just the mean of the uniform price auction data (treatment):

#convert to datatable, take the datatable, calculate the mean grouped by uniform price auction:

dt <-data.table(d2)

treat_mean <- dt[,.(mean = mean(bid)),by=uniform_price_auction]$mean[1]

#and now the non-uniform price auction (control):

control_mean <- dt[,.(mean = mean(bid)),by=uniform_price_auction]$mean[2]

#to get the True_ate, we subtract the control mean from the treatment mean:

true_ate <- treat_mean - control_mean
true_ate

```

```{r}
dt[,.(mean = mean(bid)),by=uniform_price_auction]
```

**Interesting, under the treatement of a uniform price auction, we see that the actual treatment effect is negative, that is cards in a uniform price auction sell for, on average, less than in a non-uniform price auction.**

**We recall that we can get the p-value under the randomization inference framework, by examining the percent of values that occur at a place that is smaller or larger than the p-value we observe by taking the mean of the true ATE and the distribution under the sharpe null. **

```{r}
1- mean(true_ate < results)
```



**My understanding is that to take the two-tailed p-value, we double the value of the one-tailed p-value.**

```{r}
(1- mean(true_ate < results))*2
```

**Uh oh**

g. Compute the same p-value again using analytic formulas for a two-sample t-test from your earlier statistics course. (Also see part (a).)

```{r}
t_test$p.value
```
**The P-value we get from our t-test is approximately .0588, meaning that it is marginally signficant.**

h. Compare the two p-values in parts (e) and (f). Are they much different? Why or why not? How might your answer to this question change if the sample size were different?
