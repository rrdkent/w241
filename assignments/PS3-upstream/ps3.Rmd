---
title: "Problem Set 3"
author: "Daniel Kent, Spring 2019"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
  df_print: paged
  pdf_document: default
---

<!--
Some guidelines for submitting problem sets in this course:

- Please submit a PDF document rather than a Word document or a Google document.
- Please put your name at the top of your problem set.
- Please **bold** or *highlight* your numerical answers to make them easier to find.
- If you'll be using `R` or `Python` code to calculate your answers, please put the code and its output directly into your Problem Set PDF document.
- It is highly recommended, although not required, that you use the RMarkdown feature in RStudio to compose your problem set answers. RMarkdown allows you to easily intermingle analysis code and answers in one document. It is of a similar design as `jupyter` and an ipython notebook.
- You do not need to show work for trivial calculations, but showing work is always allowed.
- For answers that involve a narrative response, please feel free to describe the key concept directly and briefly, if you can do so, and do not feel pressure to go on at length.
- Please ask us questions about the problem set if you get stuck. **Don't spend more than 20 minutes puzzling over what a problem means.** 
- Please ensure that someone (us!) can compile your solution set. The best way is to use the web-hosted links we've provided.
--> 

```{r, results='hide'} 
# load packages 
library(data.table)
library(foreign)
```

# 0 Write Functions 
You're going to be doing a few things a *number* of times -- calculating robust standard errors, calculating clustered standard errors, and then calculating the confidence intervals that are built off these standard errors. 

*After* you've worked through a few of these questions, I suspect you will see places to write a function that will do this work for you. Include those functions here, if you write them. 

```{r}

```
# 1 Replicate Results 
Skim [Broockman and Green's](http://link.springer.com/article/10.1007/s11109-013-9239-z) paper on the effects of Facebook ads and download an anonymized version of the data for Facebook users only.

```{r}
d <- read.csv("./data/broockman_green_anon_pooled_fb_users_only.csv")
``` 

**First, let's look at what we have in our data**

```{r}
head(d)
```
**Awesome, lots of data to work with, but phew, let's try to get some parameters to understand what we're working with.**

```{r}
#Load it up in Data Table!
dt <- data.table(d)
```



```{r}
dt[,.(count= .N), studyno]

```
**We observe that we're dealing with approximately 1350 observations in each study.  But let's slice it a bit more to see more detail:**

```{r}
dt[,.(count= .N), by=list(studyno, treat_ad)]
```

**OK, for Study 1, there's a 5:8 split, but with study two, there's a 10:3 split in terms of control to treatment.  Interesting and good to keep in mind.**


a. Using regression without clustered standard errors (that is, ignoring the clustered assignment), compute a confidence interval for the effect of the ad on candidate name recognition in Study 1 only (the dependent variable is "name_recall"). 
+ **Note**: Ignore the blocking the article mentions throughout this problem.
+ **Note**: You will estimate something different than is reported in the study. 

**OK, so we filter for study 1, and then run a linear regression, regressing the dependent variable (name_recall) on the indepedent variable (treat_ad).**

```{r}
dependent <- dt[c(studyno==1),,]$name_recall
independent <- dt[c(studyno==1),,]$treat_ad
```


```{r}
lm <- lm(dependent ~ independent, data=dt )
```

```{r}
lm_stat <- summary(lm)
lm_stat
```

**And using the awesome trick we learned from PS2, we can use confint to get the confidence interval:**
```{r}
confint(lm, level=.95)
```
**So we see that the confidence interval we get from the regression is (-.05 to .03).  This means that there is a 95% probability that the true ATE falls within this range.**  


b. What are the clusters in Broockman and Green's study? Why might taking clustering into account increase the standard errors?

**On page 266, the authors supply a few example clusters: "24 year old males in SF, 24 year old males in Palo Alto, 25 year old females in SF).  So consquently, it appears that they clustered on Age by Gender by Residence Location.  This is what they say on page 267: "We then generated clusters of individuals with unique combinations of age, gender, and location)."  I believe that clustering by some variables can reduce standard error by adjusting for one of the sources of deviation in our results, but may be overcome by the effect with something like when reducing the sample size for one of the cluster split conditions.  For example, if there was a discernable treatment affect among men versus women, but when clustered on men versus women, there are more men in a location than women in a location and when we adjust for one source of devation in the results by gender, we increased it by reducing the effective sample size, hypothetically speaking. **

c. Now repeat part (a), but taking clustering into account. That is, compute a confidence interval for the effect of the ad on candidate name recognition in Study 1, but now correctly accounting for the clustered nature of the treatment assignment. If you're not familiar with how to calculate these clustered and robust estimates, there is a demo worksheet that is available in our course repository: `./code/week5clusterAndRobust.Rmd`.
```{r}
#Better load up some more packages!
library(sandwich)
library(multiwayvcov)
library(lmtest)
```

```{r}
head(dt$cluster)
```


```{r}
lm$cluster1.vcov <- cluster.vcov(model = lm, cluster = dt[c(studyno==1),,]$cluster)
coeftest(lm, lm$cluster1.vcov)
```
**Now correctly accounting for the clustered nature of the treatment assignment, we get a coefficent of approximately -.01 and a SE of .023. Thus our 95% confidence interval is calculated:**

$-.01 \pm 1.96 * .0237536$

**And our 95% confidence interval is between -.9565571 and .0365571 with a distance of .0931141.**

d. Repeat part (c), but now for Study 2 only.

```{r} 
dependent <- dt[c(studyno==2),,]$name_recall
independent <- dt[c(studyno==2),,]$treat_ad
lm <- lm(dependent ~ independent, data=dt )

lm$cluster2.vcov <- cluster.vcov(model = lm, cluster = dt[c(studyno==2),,]$cluster)
coeftest(lm, lm$cluster2.vcov)
```
**Now correctly accounting for the clustered nature of the treatment assignment, we get a coefficent of approximately -.003 and a SE of .036. Thus our 95% confidence interval is calculated:**

$-.003 \pm 1.96 * .036$

**And our 95% confidence interval is between -.07356 and .06756 with a distance of .14112**

e. Repeat part (c), but using the entire sample from both studies. Do not take into account which study the data is from (more on this in a moment), but just pool the data and run one omnibus regression. What is the treatment effect estimate and associated p-value?

```{r}
dependent <- dt$name_recall
independent <- dt$treat_ad
lm <- lm(dependent ~ independent, data=dt )

lm$cluster3.vcov <- cluster.vcov(model = lm, cluster = dt$cluster)
coeftest(lm, lm$cluster3.vcov)
```
**Now correctly accounting for the clustered nature of the treatment assignment, we get a coefficent (treatment effect) of approximately -.16 and a SE of .027. Thus our 95% confidence interval is calculated:**

$-.16 \pm 1.96 * .027$

**And our 95% confidence interval is between -.21292 and -.10708 with a distance of .10584**

**The p-value associated with the treatment effect is a miniscule .000000007344**

f. Now, repeat part (e) but include a dummy variable (a 0/1 binary variable) for whether the data are from Study 1 or Study 2. What is the treatment effect estimate and associated p-value?

**I think we can create a dummy variable for study1/2 if we subtract one from the value.**

```{r}
dependent <- dt$name_recall
independent <- dt$treat_ad
lm <- lm(dependent ~ independent + (dt$studyno-1), data=dt )

lm$cluster4.vcov <- cluster.vcov(model = lm, cluster = dt$cluster)
coeftest(lm, lm$cluster4.vcov)
```
**Now correctly accounting for the clustered nature of the treatment assignment, we get a coefficent (treatment effect) of approximately .295 and a SE of .008. Thus our 95% confidence interval is calculated:**

$.295 \pm 1.96 * .008$

**And our 95% confidence interval is between .27932 and .31068 with a distance of .03136**

**The p-value associated with the treatment effect is a super-miniscule .00000000000000022**


g. Why did the results from parts (e) and (f) differ? Which result is biased, and why? (Hint: see pages 75-76 of Gerber and Green, with more detailed discussion optionally available on pages 116-121.)

**As Gerber and Green point out, estimating the overall ATE via a weighted average of each block is not equivalent to the comparison of average outcomes among all subjects in treatment and control.  When the probabilty of being assigned to the treatment or control group varies by block (which is our case), compariong the means for all subjects will yield a biased estimate of the ATE.  Thus, Part e is biased.**


h. Skim this [Facebook case study](https://www.facebook.com/notes/us-politics-on-facebook/case-study-reaching-voters-with-facebook-ads-vote-no-on-8/10150257619200882) and consider two claims they make reprinted below. Why might their results differ from Broockman and Green's? Please be specific and provide examples.

  + "There was a 19 percent difference in the way people voted in areas where Facebook Ads ran versus areas where the ads did not run."
  + "In the areas where the ads ran, people with the most online ad exposure were 17 percent more likely to vote against the proposition than those with the least."

**One issue I see is that in the Facebook Case Study, the agency targted the two most populous counties in Florida, which are Dade and Broward.  These counties may be similar to comparing apples to oranges versus the Brockman and Green study which did not select counties based upon the population.  For instance, if a county has a high population (is urban), it might stand to reason that many of these individuals would be part of a specific socio-ethnic group that could impact the study based on heterogenous treatment effects or other concepts such as computer penetration, use of social media, etc.**

**Additionally, there might be differences regarding the outcomes of what was being campagined - the Facebook Case Study was focused on a ballot initiative that impacted public schools and the Brockman and Green study was focused on specific candidates.  Thus, while a candidate might be most relevant to an entire population, the concept of public schools might be a "hot-button" issue for only those individuals who work in education or have kids who attend public schools; consequently the people may feel more pasionately/less passionately for the latter proposal versus the former.  **

**Finally, the facebook study results were presented in percentages, versus nominal ATE.  This could hoodwink the reader by causing a case of misperception through the base rate fallacy.**

# 2 Peruvian Recycling 

Look at [this article](https://drive.google.com/file/d/0BxwM1dZBYvxBVzQtQW9nbmd2NGM/view?usp=sharing) about encouraging recycling in Peru.  The paper contains two experiments, a "participation study" and a "participation intensity study."  In this problem, we will focus on the latter study, whose results are contained in Table 4 in this problem.  You will need to read the relevant section of the paper (starting on page 20 of the manuscript) in order to understand the experimental design and variables.  (*Note that "indicator variable" is a synonym for "dummy variable," in case you haven't seen this language before.*)

a. In Column 3 of Table 4A, what is the estimated ATE of providing a recycling bin on the average weight of recyclables turned in per household per week, during the six-week treatment period?  Provide a 95% confidence interval.

**We observe that by providing any bin, there is a statistically signfiicant increase in estimated ATE of average weight of recyclables turned in per week of .187 kg during the six week period.  To calculate the 95% confidence interval for the estimated ATE, we take the estimated ATE coefficent +/- two standard errors. That is:**

$0.187 \pm 1.96*(0.032) = $ **-0.12428 to 0.24972**

b. In Column 3 of Table 4A, what is the estimated ATE of sending a text message reminder on the average weight of recyclables turned in per household per week?  Provide a 95% confidence interval.

**We observe that any SMS message results in a non-statistically signficant decrease in estimated ATE of average weight of recyclables turned in per week of .024 (i.e. a -.024 gain) kg during the six week period. To calculate the 95% confidence interval for the estimated ATE, we take the estimated ATE coefficent +/- two standard errors. That is:**

$-0.024 \pm 1.96*(0.039) = $ **-.10044 to .05244**

c. Which outcome measures in Table 4A show statistically significant effects (at the 5% level) of providing a recycling bin?

**According to the Notes, any coefficent with at least two asterisks represents signficance at the 5% confidence level.  Consequently for the providing of a recycling bin (i.e. any bin), the percent of visits turned in bag, average number of bins turned in per week, average weight of recyclables turned in per week, and average market value of recyclables given per week all showed statistically signficant effects at the 5% level of providing a recycling bin. (Columns 1,2,3 and 4)**

d. Which outcome measures in Table 4A show statistically significant effects (at the 5% level) of sending text messages?

**Based on the same notes described above in part c, no outcome measured show statistically signficant effects at the 5% level when sending text messages.**

e. Suppose that, during the two weeks before treatment, household A turns in 2kg per week more recyclables than household B does, and suppose that both households are otherwise identical (including being in the same treatment group).  From the model, how much more recycling do we predict household A to have than household B, per week, during the six weeks of treatment?   Provide only a point estimate, as the confidence interval would be a bit complicated.  This question is designed to test your understanding of slope coefficients in regression.

**I believe that we want to look at the coefficent in the third column, Avg. weight of recyclables turned in per week, and the row that says Average weight of recyclables turned in per week, baseline.  Here we see a statistically signficant .281 coefficent with an SE of .011.  My understanding is that this would mean that with every incremental kg of recyclables turned in per week from the baseline, there would be an increase of that weight of recyclables times the coefficent, .281.  Consequently, there would be a 2kg * .281 or 5.62kg increase of recyclables turned in by Household A versus Household B, holding everything else constant.**

f. Suppose that the variable "percentage of visits turned in bag, baseline" had been left out of the regression reported in Column 1.  What would you expect to happen to the results on providing a recycling bin?  Would you expect an increase or decrease in the estimated ATE?  Would you expect an increase or decrease in the standard error?  Explain your reasoning.

**In column one, the variable "percentage of visits turne din bag, baseline was the highest magnitude coefficent.  I believe this means that if we turn the SMS variable (i.e. no SMS), the has cellphone (i.e. doesn't have a cellphone), and any bin(i.e. no bin) off, our baseline would be that .374 percent of visits to households resulted in the turning in of a bag.  Because this is a higher magnitude than the other variables, I believe we can deduce that this is the key driver in the estimated ATE.  If it was left out, I would suspect that some of the magnitude would be distributed proportionally to the other included variables though the overall estimated ATE would go down because we don't have as precise a model to estimate ATE.  I would also expect the Standard Error to increase because our pecision with which the regression coeffcient is esimated would be reduced.  That is, our less precise esimate of the coefficent would result in a higher SE because a major input of the response variable yielding a more precise model is removed. **

g. In column 1 of Table 4A, would you say the variable "has cell phone" is a bad control?  Explain your reasoning.

**I would say it is a non-optimal control because as the authors mentioned on page 18 themselves, that the cellphone control is correlated with SMS message assignment.  Basically, to my knowlege, since there is no other straightforward way to recieve an SMS without a cellphone, there would be a 1:1  relationship of individuals who recieve any SMS message and have a cell phone.  Consequently this might lead to interference among the coefficents because of the lack of independence of each control.**

h. If we were to remove the "has cell phone" variable from the regression, what would you expect to happen to the coefficient on "Any SMS message"?  Would it go up or down? Explain your reasoning.

**I would expect that the overall precision of the model to go down, but the coefficent on "Any SMS message" would increase, as some of the cell phone coefficent would load onto the "Any SMS Message" coefficent.  This might be because cell phones provide some explanation with respect to the participant population, but because they are highly correlated with the "any SMS message," they we would eliminate some of the interference we discussed in part g.  For instance, the cell phone might indicate other things than ability to accept SMS messages, like class (maybe more middle class residents have cell phones than lower class individuals) and this would otherwise not be included to the same degree in the regression if we eliminated the cell phone variable.  It would instead partially load onto the SMS variable, as sort of more of a catch-all for the nuance breakout we get from the cellphone variable. **

# 3 Multifactor Experiments 
Staying with the same experiment, now lets think about multifactor experiments. 

a. What is the full experimental design for this experiment?  Tell us the dimensions, such as 2x2x3.  (Hint: the full results appear in Panel 4B.)
**Bin without sticker, bin with a sticker, (didn't recieve a bin); got an generic SMS message, got a personal SMS message, (didn't get an SMS message) and *grumble* Has a Cell phone, (Doesn't have a cell phone). Consequently the dimensions are 3x3x2, one dimension regarding the bin/sticker, one regarding the SMS message, and one for the cell phone.**

b. In the results of Table 4B, describe the baseline category. That is, in English, how would you describe the attributes of the group of people for whom all dummy variables are equal to zero?

**Recognizing that we have 3x3x2 dimensions, and in Table 4B, there are only five of treatment coefficents labled, we look for what is missing.  With respect to the bin dimension, we see bin with a sticker, and bin without a sticker, so then the remaining option which would be associated with a "zero" dummy variable is "no bin" provided.  With respect to the other dimension, SMS messages, we see the personal SMS message, and generic SMS message, so the remaining option also associated with the "Zero" dummy variable is "no SMS" sent.  Finally, the last variable is "has cell phone" so the other dimension is "Doesn't have a cell phone."  So our baseline is more or less like a control - no bin, no sms, no cell phone (I.e. no treatment/interference). **

c. In column (1) of Table 4B, interpret the magnitude of the coefficient on "bin without sticker."  What does it mean?

**In column (1) of Table 4B, which is the percentage of visits turned in bag, the "bin without sticker" is associated with a coefficent of 0.035, a SE of 0.015, and two asterisks.  This means that relative to the baseline (no bin, no SMS), providing the user with just a bin without a sticker resulted in a statistically signficant average estimated treatment effect of .035 at the 5% level.  Said another way, providing a bin without a sticker made 3.5% more households turn in a bag.**

d. In column (1) of Table 4B, which seems to have a stronger treatment effect, the recycling bin with message sticker, or the recycling bin without sticker?  How large is the magnitude of the estimated difference?

**Observing the coefficents associated with the different treatmetns of "bin with sticker" and "bin without sticker" with respect to the percengae of visits where households turned in a bag, we observe that bin with sticker has a stronger treatment effect with a statistically signficant effect at the 1% level of .055 with a SE of .015, whereas the bin without a sticker has as statistically significant effect of .035 with an SE also of .015 at the 5% level.  The magnitude is the difference in ATEs since there is no interaciton term, so the magnitude difference is .02.**

e. Is this difference you just described statistically significant?  Explain which piece of information in the table allows you to answer this question.

**No, I belive that the difference is not statistically signficant because while for both bins with a sticker and without a sticker, the coefficents are associated with an independent statistical signficance at least at the 5% level, highlighted by the double and triple asterisks, we'd want to see if their confidence levels overlap - if they do, there might not be a statistically significant difference.**

$.055 \pm 1.96*(0.015)$ Conf int: .0256 and .0844
$.035 \pm 1.96*(0.015)$ Conf int: .0056 and .0644 --> Overlap between .0256 and .0644.

**Consequently, because of the standard error and the coefficent, we know that this difference is likely not statistically signficant.**

f. Notice that Table 4C is described as results from "fully saturated" models.  What does this mean?  Looking at the list of variables in the table, explain in what sense the model is "saturated."

**I believe a fully saturated model is a model that includes nearly every single possible coefficent, in a model, except for one. For example, we have No SMS message, for the bin with sticker, bin, and no bin, (all of the bin options); pesronalized SMS for the bin, bin with sticker and no bin (all of the bin options); and generic SMS message for bin and no bin.  Uh oh, so we're missing Generic SMS message with bin and sticker, so that would be the "base case".  Consequently with that in mind, everything else is included so we can see the load on each coefficent and hence we call the model saturated.**

# 4 Now! Do it with data 
Download the data set for the recycling study in the previous problem, obtained from the authors. We'll be focusing on the outcome variable Y="number of bins turned in per week" (avg_bins_treat).

```{r}
d <- read.dta("./data/karlan_data_subset_for_class.dta")
head(d)

## Do some quick exploratory data analysis with this data. There are some values in this data that seem a bit strange. Determine what these are, and figure out what you would like to do with them. Also, notice what happens with your estimates vis-a-vis the estimates that are produced by the authors when you do something sensible with this strange values. 
```

```{r}
dt <- data.table(d)
```

```{r}
dt[,.N,]
```
**OK, we're dealing with 1785 individual observations.**

```{r}
summary(dt)
```

**We see from our data that there are three NA values in the street feature, and 1 NA value in the havecell feature.  I'm recommending to drop these because I am assuming that if there is an error in the data collection for these values, the rest of the data is suspect.**
```{r}
hist(dt$street)
```
**Additionally, we see some values in Street with -999.  I am going to drop these as well for similar reasons as to the NA.**

**Everything else, generally, looks ok.**

```{r}
dt <- dt[dt$street>=0 & havecell>=0,,]
```
```{r}
summary(dt)
```
```{r}
dt[,.N,]
```


a. For simplicity, let's start by measuring the effect of providing a recycling bin, ignoring the SMS message treatment (and ignoring whether there was a sticker on the bin or not).  Run a regression of Y on only the bin treatment dummy, so you estimate a simple difference in means.  Provide a 95% confidence interval for the treatment effect.

```{r}
fit <- lm(dt$avg_bins_treat ~ dt$bin , data=dt)
summary(fit)
```

**We recall that a 95% confidence interval is the coefficent +/- 1.96 * SE So:**

$.1347 \pm 1.96 * .02086$

**The 95% confidence interval is between 0.0938144 and 0.1755856. Which means, there is a 95% probability that the true ATE falls between 0.0938144 and 0.1755856... or that this confdience interval will overlap with the population level treatment effect in 95 out of 100 times that the procedure is replicated, under the same sampling conditions.**

b. Now add the pre-treatment value of Y as a covariate.  Provide a 95% confidence interval for the treatment effect.  Explain how and why this confidence interval differs from the previous one.

```{r}
fit <- lm(dt$avg_bins_treat ~ dt$bin + dt$base_avg_bins_treat, data=dt)
summary(fit)
```


**We recall that a 95% confidence interval is the coefficent +/- 1.96 * SE So:**

$.12711 \pm 1.96 * .01722$

**The 95% confidence interval is between .0933588 and .160861. Which means, there is a 95% probability that the true ATE falls between .0933588 and .160861... or that this confdience interval will overlap with the population level treatment effect in 95 out of 100 times that the procedure is replicated, under the same sampling conditions.**

**Compared with the previous estimate, we see that our interval has decreased from .0817712 to .0675024, in addition to shifting down a bit.  This confidence interval differs in that it is smaller because we are controlling for another variable (the base value, pre-treatment of Y), so that more the explanatory magnitude falls appropriately on the variables that break out the different effects more, leading to a reduction in standard error associated with the coefficent(thereby reducing the interval) since we are dealing with a more precise model.**

c. Now add the street fixed effects.  (You'll need to use the R command factor().) Provide a 95% confidence interval for the treatment effect.  

```{r}
fit <- lm(dt$avg_bins_treat ~ dt$bin + dt$base_avg_bins_treat + factor(street), data=dt)
summary(fit)
```

d. Recall that the authors described their experiment as "stratified at the street level," which is a synonym for blocking by street.  Explain why the confidence interval with fixed effects does not differ much from the previous one.

**Our previous model, which included measuring the effect of providing a recycling bin and the pre-treatment value of Y as a covariate, resulted in a statistically significant coefficent of .12711 and an SE of .01722.  Our model stratified at the street level resulted in a statistically signficant coefficent for the treatment of .1160641 and an SE of .0176099.  The previous model's 95% confidence interval was between .0933588 and .160861 with a distance of .0675024 and this model, with street effects, is:**

$.1160641 \pm 1.96 * .0176099$
 **Between .0815487 amd .15058 with a distance of .0690308.  I believe that the confidence interval doesn't differ much from the previous model because not much information is being added when we add the blocking by street protocol.  In effect, our model is not getting more precies thereby reducing our standard errors yielding a smaller confidence interval.**

e. Perhaps having a cell phone helps explain the level of recycling behavior. Instead of "has cell phone," we find it easier to interpret the coefficient if we define the variable " no cell phone."  Give the R command to define this new variable, which equals one minus the "has cell phone" variable in the authors' data set.  Use "no cell phone" instead of "has cell phone" in subsequent regressions with this dataset.

```{r}
output <- 1-dt$havecell
head(output)
```
```{r}
head(dt$havecell)
```
```{r}
dt$nocell <- output
dt
```

```{r}
head(dt$nocell)
```


f. Now add "no cell phone" as a covariate to the previous regression.  Provide a 95% confidence interval for the treatment effect.  Explain why this confidence interval does not differ much from the previous one.

```{r}
fit <- lm(dt$avg_bins_treat ~ dt$bin + dt$base_avg_bins_treat + dt$nocell + factor(street), data=dt)
summary(fit)
```

**Our previous, model stratified at the street level resulted in a statistically signficant coefficent for the treatment of .1160641 and an SE of .0176099.  This yields a 95% confidence interval of between .0815487 amd .15058 with a distance of .0690308**

**Our current model with the nocell covariate yields a statistically signficant treatment coefficent of .1171694 and an SE of .0175861**

$.1171694 \pm 1.96 * .0175861$

**Yielding a 95% confidence interval of between .0827006 and .151638 with a distance of .0689375, which is a hare smaller of an interval than from the previous model.**

**Again, I believe that the confidence interval doesn't differ much from the previous model because not much information is being added when we adding the no-cell covariate.  In effect, our model is not getting more precise because much of the explanatory power of the coefficents is not broken out by the nocell covariate.**

g. Now let's add in the SMS treatment.  Re-run the previous regression with "any SMS" included.  You should get the same results as in Table 4A.  Provide a 95% confidence interval for the treatment effect of the recycling bin.  Explain why this confidence interval does not differ much from the previous one.

```{r}
fit <- lm(dt$avg_bins_treat ~ dt$bin + dt$base_avg_bins_treat + dt$nocell + dt$sms + factor(street), data=dt)
summary(fit)
```

h. Now reproduce the results of column 2 in Table 4B, estimating separate treatment effects for the two types of SMS treatments and the two types of recycling-bin treatments.  Provide a 95% confidence interval for the effect of the unadorned recycling bin.  Explain how your answer differs from that in part (g), and explain why you think it differs.

```{r}
fit <- lm(dt$avg_bins_treat ~ dt$sms_g + dt$sms_p + dt$bin_g + dt$bin_s, data=dt)
summary(fit)
```

**We obtain a 95% confidence level by taking the sum of the coefficent being examined plus/minus 1.96 times the Standard Error.  Because the unadorned recycling bin's Coefficent is .130793 and its SE is .026975:**

$.130793 \pm 1.96 * .026975$

**Our 95% confidence interval is between 0.077922 and 0.183664, a distance of .105742.  I believe this model differes because we are breaking out the discerete treatments by their type (generic versus personal SMS, bin sticker versus bin no-sticker), which together yield more context and specific data regarding different outcomes.  Consequently this estimate will have a higher accuracy than the other model.  **

# 5 A Final Practice Problem 
Now for a fictional scenario. An emergency two-week randomized controlled trial of the experimental drug ZMapp is conducted to treat Ebola. (The control represents the usual standard of care for patients identified with Ebola, while the treatment is the usual standard of care plus the drug.) 

Here are the (fake) data. 

```{r}
d <- read.csv("./data/ebola_rct2.csv")
head(d)
```

```{r}
dt <- data.table(d)
dt[,.N,]
```
 **OK, we have 100 observations**

You are asked to analyze it. Patients' temperature and whether they are vomiting is recorded on day 0 of the experiment, then ZMapp is administered to patients in the treatment group on day 1. Vomiting and temperature is again recorded on day 14.

a. Without using any covariates, answer this question with regression: What is the estimated effect of ZMapp (with standard error in parentheses) on whether someone was vomiting on day 14? What is the p-value associated with this estimate?

```{r}
fit <- lm(dt$vomiting_day14 ~ dt$treat_zmapp , data=dt)
summary(fit)
```

**The estimated treatment effect of ZMapp on whether someone was vomiting on day 14 was -0.23770 (SE is .08563).  The P-value associated with this esimate is .0066, which is significant at the 1% level. **

b. Add covariates for vomiting on day 0 and patient temperature on day 0 to the regression from part (a) and report the ATE (with standard error). Also report the p-value.

```{r}
fit <- lm(dt$vomiting_day14 ~ dt$treat_zmapp + dt$vomiting_day0 + dt$temperature_day0, data=dt)
summary(fit)
```

**The estimated treatment effect of ZMapp on whether someone was vomiting on day 14 with this expanded model was -0.16554 (SE is .07567).  The P-value associated with this esimate is .03113, which is significant at the 5% level. **

c. Do you prefer the estimate of the ATE reported in part (a) or part (b)? Why?

**I prefer the estimate of the ATE reported in part b versus part a, because we are adding covariates that logically make sense that they would have an impact on the response variable - that is, the vomiting of a subject on day 14 would be impacted by other features than just if she took ZMapp.  For instance, if she was vomitting in Day 0, i.e. indicating illness, it could have a direct effect (without treatment) that she would still be sick and vomitting on day 14.  Further, another indicator of illness would be temperature pointing to a fever, which would also play into whether one is vomiting on day 14.  Further, our adjusted R-squared seems to have increased as well indicating a better fit model. **

d. The regression from part (b) suggests that temperature is highly predictive of vomiting. Also include temperature on day 14 as a covariate in the regression from part (b) and report the ATE, the standard error, and the p-value.

```{r}
fit <- lm(dt$vomiting_day14 ~ dt$treat_zmapp + dt$vomiting_day0 + dt$temperature_day0 + dt$temperature_day14, data=dt)
summary(fit)
```

**The estimated treatment effect of ZMapp on whether someone was vomiting on day 14 with this expanded model was -0.12010 (SE is 0.07768).  The P-value associated with this esimate is .12541, which is not signficant. **

e. Do you prefer the estimate of the ATE reported in part (b) or part (d)? Why?

**Were I a greedy fishing scientist, I would prefer the estimate of the ATE reported in part b, becauase the results indicated a statistically signficant treatment effect; however, as a dispassionate and emperically-minded scientist, I prefer the estimate of the ATE in part d, because it presents a more complete model (The Adjusted R-squared increased a bit), and as we discussed above, temperature on day 14 makes logical sense that it might be an indicator of illness yielding vomitting on day 14.  Consqeuently, even though we lose our statistically significant coefficent, I believe it presents a more robust model and picture of what is going on. **

f. Now let's switch from the outcome of vomiting to the outcome of temperature, and use the same regression covariates as in part (b). Test the hypothesis that ZMapp is especially likely to reduce men's temperatures, as compared to women's, and describe how you did so. What do the results suggest?

```{r}
fit <- lm(dt$temperature_day14 ~ dt$treat_zmapp + dt$vomiting_day0 + dt$temperature_day0 +factor(dt$male), data=dt)
summary(fit)
```

g. Suppose that you had not run the regression in part (f). Instead, you speak with a colleague to learn about heterogenous treatment effects. This colleague has access to a non-anonymized version of the same dataset and reports that he had looked at heterogenous effects of the ZMapp treatment by each of 10,000 different covariates to examine whether each predicted the effectiveness of ZMapp on each of 2,000 different indicators of health, for 20,000,000 different regressions in total. Across these 20,000,000 regressions your colleague ran, the treatment's interaction with gender on the outcome of temperature is the only heterogenous treatment effect that he found to be statistically significant. He reasons that this shows the importance of gender for understanding the effectiveness of the drug, because nothing else seemed to indicate why it worked. Bolstering his confidence, after looking at the data, he also returned to his medical textbooks and built a theory about why ZMapp interacts with processes only present in men to cure. Another doctor, unfamiliar with the data, hears his theory and finds it plausible. How likely do you think it is ZMapp works especially well for curing Ebola in men, and why? (This question is conceptual can be answered without performing any computation.)

**A large issue of mine is that we don't know how the sample was selected - whether it was a randomized trial or whether it was given to select individuals, such as US military personnel.  Just like the HIE from the Mastering 'Metrics book which had treatment groups associated with each plan were mostly too small for comparisons between them to be statistically meaningful, an additional concern of mine is that we are dealing with a sample of 100 individuals, which, in the scheme of things, is not that large.  Gerber and Green mention that when we compare treatment affects across different subgroups and settings, we have to ensure that our interpretation is done in a rigerous manner so it is not impacated by things like sampling variability, which is my concern.  I also take issue with his logic-by-deduction - that because the only heterogenous treatment effect that was statistically signficant was gender, then it *must* be gender**

```{r}
dt[,.N,by=list(male, treat_zmapp)]
```
**Even when our sample size is 100, I am even more worried about the sample of this study and in particular, the distribution of subjects - 63/100 were women, where 23 of the 63 got the ZMapp treatment, and only 37 of the subjects were men, where 18 got the treatment.  My gut and intuition tells me that these groupings are too small for us to comfortably draw such conclusions.  **

**Perhaps were we to develop and pre-specify the investigating of conditional average treatment effects for these two subgroups, I would be more inclined to support the small sample sizes, but this retrospective look makes this seem potentially brash to consider post-hoc.**

**Finally, I feel like we are on a grand fishing expedition.  Just like Gerber and Green mention on page 300, we are in for a multiple comparisons problem where if we have enough subgroups, odds are good that statistically signficant interactions with at least one covariate (!) will pop up merely by chance, and this is potentially what we're observing.   **

h. Now, imagine that what described in part (g) did not happen, but that you had tested this heterogeneous treatment effect, and only this heterogeneous treatment effect, of your own accord. Would you be more or less inclined to believe that the heterogeneous treatment effect really exists? Why?

**I feel like my concerns from above still hold; that our sample size is too small to draw such a conclusion about the heterogeneous treatment effect and we should have developed a CATE protocol prior to running the study if we assumed as such.  This being said, I feel like I would be less concerned about the multiple comparisons problem though.**

i. Another colleague proposes that being of African descent causes one to be more likely to get Ebola. He asks you what ideal experiment would answer this question. What would you tell him?  (*Hint: refer to Chapter 1 of Mostly Harmless Econometrics.*)

**An ideal experiment would consist of blocking based upon ethnic heritage, including a block of individuals that were of African descent.  But as Gerber and Green point out at the bottom of page 301, a more fundamental limitation of subgroup analysis  is that it is essentially non-experimental in character.  It could be that being of African descent is merely a marker for other factors.  The fact of being of African descent may predict the size of the treatment effect, but testing/blocking on descent may not cause an change in the size of the conditional average treatment effect. **